{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1549,
     "status": "ok",
     "timestamp": 1647663386144,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "YTahRQDlBVa2",
    "outputId": "ba9628f3-8d15-4185-9a6e-afea791657e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1647663387041,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "gRGtvG7nqqvr",
    "outputId": "281d46ea-f2f5-40f9-b085-1b30e26c3e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_1d.npy\t\t\ty_grain_cond_ka_test.npy\n",
      "X_train_1d.npy\t\t\ty_grain_cond_ka_train.npy\n",
      "X_val_1d.npy\t\t\ty_grain_cond_ka_val.npy\n",
      "y_grain_cond_filt_ka_test.npy\ty_raw_ka_test.npy\n",
      "y_grain_cond_filt_ka_train.npy\ty_raw_ka_train.npy\n",
      "y_grain_cond_filt_ka_val.npy\ty_raw_ka_val.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "!ls /content/drive/MyDrive/PML_project/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1647663387043,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "KHqtulMoBsVz"
   },
   "outputs": [],
   "source": [
    "path = 'dataset/'\n",
    "X_train = np.load(path+'X_train_1d.npy')\n",
    "X_test = np.load(path+'X_test_1d.npy')\n",
    "X_val = np.load(path+'X_val_1d.npy') #this may not be required for now\n",
    "\n",
    "y_train = np.load(path+'y_grain_cond_ka_train.npy')\n",
    "y_train = np.where(y_train>=0, 1, 0)\n",
    "y_test = np.load(path+'y_grain_cond_ka_test.npy')\n",
    "y_test = np.where(y_test>=0, 1, 0)\n",
    "y_val = np.load(path+'y_grain_cond_ka_val.npy') #this may not be required for now\n",
    "y_val = np.where(y_val>=0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1647663387044,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "2VwlWzcID60F",
    "outputId": "31a47f5a-0eab-4da0-f5f0-8bd90625fe43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3185, 4) (970, 4) (676, 4)\n",
      "(3185,) (970,) (676,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(y_train.shape, y_test.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1647663387045,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "fdu9knJU5JoG"
   },
   "outputs": [],
   "source": [
    "del_idx = np.unique(np.where(np.isnan(X_train))[0])\n",
    "del_idx_test = np.unique(np.where(np.isnan(X_test))[0])\n",
    "X_train = np.delete(X_train, del_idx, axis=0)\n",
    "X_test = np.delete(X_test, del_idx_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1647663387246,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "xkpgOOuC6Anb"
   },
   "outputs": [],
   "source": [
    "y_train = np.delete(y_train, del_idx, axis=0)\n",
    "y_test = np.delete(y_test, del_idx_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1647663387246,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "tz9t69w86LIS",
    "outputId": "29b98138-23ac-467f-d735-8e6a7d89abef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2074, 4) (632, 4) (676, 4)\n",
      "(2074,) (632,) (676,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(y_train.shape, y_test.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 33112,
     "status": "error",
     "timestamp": 1647663420352,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "0ul-e3AsI51A",
    "outputId": "afc08f72-8f23-4983-fe5e-a5d8e7697b19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/util.py:102: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  trainable=trainable)\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/util.py:112: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  trainable=trainable)\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: drive/MyDrive/PML_project/blr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0319 04:16:44.157793 140027568953216 builder_impl.py:780] Assets written to: drive/MyDrive/PML_project/blr/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 4s 4ms/step - loss: 0.6960 - binary_accuracy: 0.5497\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6826 - binary_accuracy: 0.6133\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6772 - binary_accuracy: 0.5849\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6714 - binary_accuracy: 0.5873\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6784 - binary_accuracy: 0.5853\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6748 - binary_accuracy: 0.5824\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6736 - binary_accuracy: 0.5950\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6764 - binary_accuracy: 0.5969\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6739 - binary_accuracy: 0.6061\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6739 - binary_accuracy: 0.5993\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6730 - binary_accuracy: 0.5945\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6673 - binary_accuracy: 0.6143\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6747 - binary_accuracy: 0.6128\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6684 - binary_accuracy: 0.6109\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6730 - binary_accuracy: 0.6148\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6681 - binary_accuracy: 0.6167\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6749 - binary_accuracy: 0.6061\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6664 - binary_accuracy: 0.6109\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6716 - binary_accuracy: 0.5979\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6666 - binary_accuracy: 0.6249\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6699 - binary_accuracy: 0.6095\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6723 - binary_accuracy: 0.5984\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6662 - binary_accuracy: 0.6186\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6657 - binary_accuracy: 0.6109\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6735 - binary_accuracy: 0.6186\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6702 - binary_accuracy: 0.6162\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6642 - binary_accuracy: 0.6152\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6725 - binary_accuracy: 0.6056\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6706 - binary_accuracy: 0.6210\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6702 - binary_accuracy: 0.6162\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6713 - binary_accuracy: 0.5988\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6684 - binary_accuracy: 0.5988\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6659 - binary_accuracy: 0.6119\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6716 - binary_accuracy: 0.6196\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6736 - binary_accuracy: 0.6061\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6723 - binary_accuracy: 0.5979\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6693 - binary_accuracy: 0.6090\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6698 - binary_accuracy: 0.6157\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6663 - binary_accuracy: 0.6244\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6692 - binary_accuracy: 0.6225\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6693 - binary_accuracy: 0.6017\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6667 - binary_accuracy: 0.6123\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6657 - binary_accuracy: 0.6167\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6676 - binary_accuracy: 0.6302\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6706 - binary_accuracy: 0.6157\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6677 - binary_accuracy: 0.6167\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6685 - binary_accuracy: 0.6152\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6650 - binary_accuracy: 0.6061\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6661 - binary_accuracy: 0.6046\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6700 - binary_accuracy: 0.6205\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6708 - binary_accuracy: 0.6109\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6698 - binary_accuracy: 0.5993\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6686 - binary_accuracy: 0.6114\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6717 - binary_accuracy: 0.6070\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6705 - binary_accuracy: 0.6143\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6680 - binary_accuracy: 0.6167\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6703 - binary_accuracy: 0.6143\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6677 - binary_accuracy: 0.6205\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6669 - binary_accuracy: 0.6152\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6653 - binary_accuracy: 0.6051\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6685 - binary_accuracy: 0.6128\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6679 - binary_accuracy: 0.6152\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6695 - binary_accuracy: 0.6172\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6671 - binary_accuracy: 0.6186\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6662 - binary_accuracy: 0.6297\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6685 - binary_accuracy: 0.6099\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6640 - binary_accuracy: 0.6123\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6708 - binary_accuracy: 0.6095\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6693 - binary_accuracy: 0.6133\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6662 - binary_accuracy: 0.6176\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6676 - binary_accuracy: 0.6099\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6707 - binary_accuracy: 0.6128\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6672 - binary_accuracy: 0.6085\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6674 - binary_accuracy: 0.6109\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6688 - binary_accuracy: 0.6172\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6666 - binary_accuracy: 0.6205\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6671 - binary_accuracy: 0.6056\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6686 - binary_accuracy: 0.6157\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6683 - binary_accuracy: 0.6119\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6689 - binary_accuracy: 0.6099\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6709 - binary_accuracy: 0.6186\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6683 - binary_accuracy: 0.6234\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6686 - binary_accuracy: 0.6075\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6677 - binary_accuracy: 0.6205\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6708 - binary_accuracy: 0.6008\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6717 - binary_accuracy: 0.5993\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6686 - binary_accuracy: 0.6041\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6686 - binary_accuracy: 0.6152\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6673 - binary_accuracy: 0.6075\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6687 - binary_accuracy: 0.6114\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6683 - binary_accuracy: 0.6186\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6655 - binary_accuracy: 0.6128\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6689 - binary_accuracy: 0.5979\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6706 - binary_accuracy: 0.6070\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6675 - binary_accuracy: 0.6119\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6660 - binary_accuracy: 0.6090\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6700 - binary_accuracy: 0.6027\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6683 - binary_accuracy: 0.6070\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6689 - binary_accuracy: 0.6138\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6683 - binary_accuracy: 0.6066\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_flipout (DenseFlipout  (None, 1)                10        \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from matplotlib import cm\n",
    "from matplotlib import figure\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "flags.DEFINE_float('learning_rate',\n",
    "                   default=0.01,\n",
    "                   help='Initial learning rate.')\n",
    "flags.DEFINE_integer('num_epochs',\n",
    "                     default=100,\n",
    "                     help='Number of epochs to run.')\n",
    "flags.DEFINE_integer('batch_size',\n",
    "                     default=128,\n",
    "                     help='Batch size.')\n",
    "flags.DEFINE_string(\n",
    "    'model_dir',\n",
    "    default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                         'logistic_regression/'),\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "flags.DEFINE_integer('num_examples',\n",
    "                     default=2074,\n",
    "                     help='Number of datapoints to generate.')\n",
    "flags.DEFINE_integer('num_monte_carlo',\n",
    "                     default=50,\n",
    "                     help='Monte Carlo samples to visualize weight posterior.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "# The dimensions of the example data, ie shape=(256, 2)\n",
    "NUM_DIMENSIONS = 4\n",
    "\n",
    "MODEL = None\n",
    "\n",
    "####Delete all flags before declare#####\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname):\n",
    "  \"\"\"Utility method to visualize decision boundaries in R^2.\n",
    "  Args:\n",
    "    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n",
    "    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n",
    "    label for each point.\n",
    "    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n",
    "    shape `[2]` and `b` is a scalar `float`, interpreted as a\n",
    "    decision rule of the form `dot(features, w) + b > 0`.\n",
    "    candidate_w_bs: Python `iterable` containing tuples of the same form as\n",
    "    true_w_b.\n",
    "    fname: The filename to save the plot as a PNG image (Python `str`).\n",
    "  \"\"\"\n",
    "  fig = figure.Figure(figsize=(6, 6))\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.scatter(features[:, 0], features[:, 1],\n",
    "             c=np.float32(labels[:, 0]),\n",
    "             cmap=cm.get_cmap('binary'),\n",
    "             edgecolors='k')\n",
    "\n",
    "  def plot_weights(w, b, **kwargs):\n",
    "    w1, w2, w3, w4 = w\n",
    "    x1s = np.linspace(-1, 1, 100)\n",
    "    x2s = -(w1  * x1s + b) / w2\n",
    "    ax.plot(x1s, x2s, **kwargs)\n",
    "\n",
    "  for w, b in candidate_w_bs:\n",
    "    plot_weights(w, b,\n",
    "                 alpha=1./np.sqrt(len(candidate_w_bs)),\n",
    "                 lw=1, color='blue')\n",
    "\n",
    "  if true_w_b is not None:\n",
    "    plot_weights(*true_w_b, lw=4,\n",
    "                 color='green', label='true separator')\n",
    "\n",
    "  ax.set_xlim([-1.5, 1.5])\n",
    "  ax.set_ylim([-1.5, 1.5])\n",
    "  ax.legend()\n",
    "\n",
    "  canvas.print_figure(fname, format='png')\n",
    "  print('saved {}'.format(fname))\n",
    "\n",
    "\n",
    "def toy_logistic_data(num_examples, input_size=2, weights_prior_stddev=5.0):\n",
    "  \"\"\"Generates synthetic data for binary classification.\n",
    "  Args:\n",
    "    num_examples: The number of samples to generate (scalar Python `int`).\n",
    "    input_size: The input space dimension (scalar Python `int`).\n",
    "    weights_prior_stddev: The prior standard deviation of the weight\n",
    "        vector. (scalar Python `float`).\n",
    "  Returns:\n",
    "    random_weights: Sampled weights as a Numpy `array` of shape\n",
    "        `[input_size]`.\n",
    "    random_bias: Sampled bias as a scalar Python `float`.\n",
    "    design_matrix: Points sampled uniformly from the cube `[-1,\n",
    "        1]^{input_size}`, as a Numpy `array` of shape `(num_examples,\n",
    "        input_size)`.\n",
    "    labels: Labels sampled from the logistic model `p(label=1) =\n",
    "        logistic(dot(features, random_weights) + random_bias)`, as a Numpy\n",
    "        `int32` `array` of shape `(num_examples, 1)`.\n",
    "  \"\"\"\n",
    "  random_weights = weights_prior_stddev * np.random.randn(input_size)\n",
    "  random_bias = np.random.randn()\n",
    "  design_matrix = np.random.rand(num_examples, input_size) * 2 - 1\n",
    "  logits = np.reshape(\n",
    "      np.dot(design_matrix, random_weights) + random_bias,\n",
    "      (-1, 1))\n",
    "  p_labels = 1. / (1 + np.exp(-logits))\n",
    "  labels = np.int32(p_labels > np.random.rand(num_examples, 1))\n",
    "  return random_weights, random_bias, np.float32(design_matrix), labels\n",
    "\n",
    "def real_logistic_data(num_examples, input_size=4):\n",
    "  random_weights = np.random.randn(input_size)\n",
    "  random_bias = np.random.randn()\n",
    "  design_matrix = X_train\n",
    "  labels = y_train.reshape((-1,1))\n",
    "  return random_weights, random_bias, np.float32(design_matrix), labels\n",
    "\n",
    "class ToyDataSequence(tf.keras.utils.Sequence):\n",
    "  \"\"\"Creates a sequence of labeled points from provided numpy arrays.\"\"\"\n",
    "\n",
    "  def __init__(self, features, labels, batch_size):\n",
    "    \"\"\"Initializes the sequence.\n",
    "    Args:\n",
    "      features: Numpy `array` of features, indexed by the first dimension.\n",
    "      labels: Numpy `array` of features, with the same first dimension as\n",
    "              `features`.\n",
    "      batch_size: Integer, number of elements in each training batch.\n",
    "    \"\"\"\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(np.ceil(len(self.features) / self.batch_size))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_x = self.features[self.batch_size * idx : self.batch_size * (idx + 1)]\n",
    "    batch_y = self.labels[self.batch_size * idx: self.batch_size * (idx + 1)]\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "def create_model(num_samples, num_dimensions):\n",
    "  \"\"\"Creates a Keras model for logistic regression.\n",
    "  Args:\n",
    "   num_samples: Integer for number of training samples.\n",
    "   num_dimensions: Integer for number of features in dataset.\n",
    "  Returns:\n",
    "    model: Compiled Keras model.\n",
    "  \"\"\"\n",
    "  # KL divergence weighted by the number of training samples, using\n",
    "  # lambda function to pass as input to the kernel_divergence_fn on\n",
    "  # flipout layers.\n",
    "  kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                            tf.cast(num_samples, dtype=tf.float32))\n",
    "\n",
    "  # Define a logistic regression model as a Bernoulli distribution\n",
    "  # parameterized by logits from a single linear layer. We use the Flipout\n",
    "  # Monte Carlo estimator for the layer: this enables lower variance\n",
    "  # stochastic gradients than naive reparameterization.\n",
    "  input_layer = tf.keras.layers.Input(shape=num_dimensions)\n",
    "  dense_layer = tfp.layers.DenseFlipout(\n",
    "      units=1,\n",
    "      activation='sigmoid',\n",
    "      kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),\n",
    "      bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(),\n",
    "      kernel_divergence_fn=kl_divergence_function)(input_layer)\n",
    "\n",
    "  # Model compilation.\n",
    "  model = tf.keras.Model(inputs=input_layer, outputs=dense_layer)\n",
    "  optimizer = tf.keras.optimizers.Adam(lr=FLAGS.learning_rate)\n",
    "  # We use the binary_crossentropy loss since this toy example contains\n",
    "  # two labels. The Keras API will then automatically add the\n",
    "  # Kullback-Leibler divergence (contained on the individual layers of\n",
    "  # the model), to the cross entropy loss, effectively\n",
    "  # calcuating the (negated) Evidence Lower Bound Loss (ELBO)\n",
    "  model.compile(optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "  #del_all_flags(tf.flags.FLAGS)\n",
    "  del argv\n",
    "  if tf.io.gfile.exists(FLAGS.model_dir):\n",
    "    print('Warning: deleting old log directory at {}'.format(FLAGS.model_dir))\n",
    "    tf.io.gfile.rmtree(FLAGS.model_dir)\n",
    "  tf.io.gfile.makedirs(FLAGS.model_dir)\n",
    "\n",
    "  # Generate a toy classification dataset.\n",
    "  w_true, b_true, features, labels = real_logistic_data(FLAGS.num_examples)\n",
    "  # w_true, b_true, features, labels = toy_logistic_data(FLAGS.num_examples)\n",
    "  toy_logistic_sequence = ToyDataSequence(features, labels, FLAGS.batch_size)\n",
    "\n",
    "  # Define and train a bayesian logistic regression model.\n",
    "  model = create_model(FLAGS.num_examples, NUM_DIMENSIONS)\n",
    "  model.save('drive/MyDrive/PML_project/blr')\n",
    "  model.fit(toy_logistic_sequence, epochs=FLAGS.num_epochs,\n",
    "            shuffle=True)\n",
    "  print(model.summary())\n",
    "  MODEL = model\n",
    "  # return model\n",
    "  # Visualize some draws from the weights posterior.\n",
    "  # candidate_w_bs = [(model.layers[-1].kernel_posterior.sample().numpy(),\n",
    "  #                    model.layers[-1].bias_posterior.sample().numpy())\n",
    "  #                   for _ in range(FLAGS.num_monte_carlo)]\n",
    "  # visualize_decision(features, labels, (w_true, b_true),\n",
    "  #                    candidate_w_bs,\n",
    "  #                    fname=os.path.join(FLAGS.model_dir,\n",
    "  #                                       'weights_inferred.png'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1647663426332,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "4qcngxrD_wu3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('drive/MyDrive/PML_project/blr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1647663426839,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "KyamlUl-AH39"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_bin = np.where(y_pred>=0.5,1,0)\n",
    "def accuracy(y_true, y_pred):\n",
    "  count = 0\n",
    "  for i in range(len(y_true)):\n",
    "    if y_true[i] == y_pred[i]:\n",
    "      count += 1\n",
    "  return count/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1647663498100,
     "user": {
      "displayName": "vignesh babu rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18089306629986300486"
     },
     "user_tz": 360
    },
    "id": "4kVphU5wJyBE",
    "outputId": "98dcf051-47b5-4c3d-d3c5-054e759f8c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6218354430379747"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred_bin.ravel())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BayesianLogisticRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
